class(k) <- "kernel"
k
}
n = 25
require('kernlab')
kfunction <- function(linear =0, quadratic=0)
{
k <- function (x,y)
{
linear*sum((x)*(y)) + quadratic*sum((x^2)*(y^2))
}
class(k) <- "kernel"
k
}
n = 25
a1 = rnorm(n)
a2 = 1 - a1 + 2* runif(n)
b1 = rnorm(n)
b2 = -1 - b1 - 2*runif(n)
x = rbind(matrix(cbind(a1,a2),,2),matrix(cbind(b1,b2),,2))
y <- matrix(c(rep(1,n),rep(-1,n)))
svp <- ksvm(x,y,type="C-svc",C = 100, kernel=kfunction(1,0),scaled=c())
plot(c(min(x[,1]), max(x[,1])),c(min(x[,2]), max(x[,2])),type='n',xlab='x1',ylab='x2')
title(main='Linear Separable Features')
ymat <- ymatrix(svp)
points(x[-SVindex(svp),1], x[-SVindex(svp),2], pch = ifelse(ymat[-SVindex(svp)] < 0, 2, 1))
points(x[SVindex(svp),1], x[SVindex(svp),2], pch = ifelse(ymat[SVindex(svp)] < 0, 17, 16))
# Extract w and b from the model
w <- colSums(coef(svp)[[1]] * x[SVindex(svp),])
b <- b(svp)
# Draw the lines
abline(b/w[2],-w[1]/w[2])
abline((b+1)/w[2],-w[1]/w[2],lty=2)
abline((b-1)/w[2],-w[1]/w[2],lty=2)
require('kernlab')
kfunction <- function(linear =0, quadratic=0)
{
k <- function (x,y)
{
linear*sum((x)*(y)) + quadratic*sum((x^2)*(y^2))
}
class(k) <- "kernel"
k
}
n = 20
r = runif(n)
a = 2*pi*runif(n)
a1 = r*sin(a)
a2 = r*cos(a)
r = 2+runif(n)
a = 2*pi*runif(n)
b1 = r*sin(a)
b2 = r*cos(a)
x = rbind(matrix(cbind(a1,a2),,2),matrix(cbind(b1,b2),,2))
y <- matrix(c(rep(1,n),rep(-1,n)))
svp <- ksvm(x,y,type="C-svc",C = 100, kernel=kfunction(0,1),scaled=c())
par(mfrow=c(1,2))
plot(c(min(x[,1]), max(x[,1])),c(min(x[,2]), max(x[,2])),type='n',xlab='x1',ylab='x2')
title(main='Feature Space')
ymat <- ymatrix(svp)
points(x[-SVindex(svp),1], x[-SVindex(svp),2], pch = ifelse(ymat[-SVindex(svp)] < 0, 2, 1))
points(x[SVindex(svp),1], x[SVindex(svp),2], pch = ifelse(ymat[SVindex(svp)] < 0, 17, 16))
# Extract w and b from the model
w2 <- colSums(coef(svp)[[1]] * x[SVindex(svp),]^2)
b <- b(svp)
x1 = seq(min(x[,1]),max(x[,1]),0.01)
x2 = seq(min(x[,2]),max(x[,2]),0.01)
points(-sqrt((b-w2[1]*x2^2)/w2[2]), x2, pch = 16 , cex = .1 )
points(sqrt((b-w2[1]*x2^2)/w2[2]), x2, pch = 16 , cex = .1 )
points(x1, sqrt((b-w2[2]*x1^2)/w2[1]), pch = 16 , cex = .1 )
points(x1,  -sqrt((b-w2[2]*x1^2)/w2[1]), pch = 16, cex = .1 )
points(-sqrt((1+ b-w2[1]*x2^2)/w2[2]) , x2, pch = 16 , cex = .1 )
points( sqrt((1 + b-w2[1]*x2^2)/w2[2]) , x2,  pch = 16 , cex = .1 )
points( x1 , sqrt(( 1 + b -w2[2]*x1^2)/w2[1]), pch = 16 , cex = .1 )
points( x1 , -sqrt(( 1 + b -w2[2]*x1^2)/w2[1]), pch = 16, cex = .1 )
points(-sqrt((-1+ b-w2[1]*x2^2)/w2[2]) , x2, pch = 16 , cex = .1 )
points( sqrt((-1 + b-w2[1]*x2^2)/w2[2]) , x2,  pch = 16 , cex = .1 )
points( x1 , sqrt(( -1 + b -w2[2]*x1^2)/w2[1]), pch = 16 , cex = .1 )
points( x1 , -sqrt(( -1 + b -w2[2]*x1^2)/w2[1]), pch = 16, cex = .1 )
xsq <- x^2
svp <- ksvm(xsq,y,type="C-svc",C = 100, kernel=kfunction(1,0),scaled=c())
plot(c(min(xsq[,1]), max(xsq[,1])),c(min(xsq[,2]), max(xsq[,2])),type='n',xlab='x1^2',ylab='x2^2')
title(main='Quadratic Kernel Space')
ymat <- ymatrix(svp)
points(xsq[-SVindex(svp),1], xsq[-SVindex(svp),2], pch = ifelse(ymat[-SVindex(svp)] < 0, 2, 1))
points(xsq[SVindex(svp),1], xsq[SVindex(svp),2], pch = ifelse(ymat[SVindex(svp)] < 0, 17, 16))
# Extract w and b from the model
w <- colSums(coef(svp)[[1]] * xsq[SVindex(svp),])
b <- b(svp)
# Draw the lines
abline(b/w[2],-w[1]/w[2])
abline((b+1)/w[2],-w[1]/w[2],lty=2)
abline((b-1)/w[2],-w[1]/w[2],lty=2)
library(tm)
## get a sample (10 documents) of the Reuters dataset (comes with package tm)
reut21578 <- system.file("texts", "crude", package = "tm")
#
reuters <- Corpus(DirSource(reut21578),
readerControl = list(reader = readReut21578XML))
### download reuters21578 data first (use first 1000 documents; 1984/85)
file <- "reut2-000.xml"
reuters <- Corpus(ReutersSource(file), readerControl = list(reader = readReut21578XML))
reuters
reuters[[1]]
## Convert to Plain Text Documents
reuters <- tm_map(reuters, as.PlainTextDocument)
reuters[[1]]
## Convert to Lower Case
reuters <- tm_map(reuters, tolower)
reuters[[1]]
## Remove Stopwords
reuters <- tm_map(reuters, removeWords, stopwords("english"))
reuters[[1]]
## Remove Punctuations
reuters <- tm_map(reuters, removePunctuation)
reuters[[1]]
## Stemming
reuters <- tm_map(reuters, stemDocument)
reuters[[1]]
## Remove Numbers
reuters <- tm_map(reuters, removeNumbers)
reuters[[1]]
## Eliminating Extra White Spaces
reuters <- tm_map(reuters, stripWhitespace)
reuters[[1]]
## create a term document matrix
dtm <- DocumentTermMatrix(reuters)
inspect(dtm[1:10, 5001:5010])
findFreqTerms(dtm, 100)
findAssocs(dtm, "washington", .4)
#washington  secretari  political     reagan republican      white      regan
#      1.00       0.49       0.46       0.45       0.45       0.42       0.41
#staff strategist
#0.41       0.41
## do tfxidf
dtm_tfxidf <- weightTfIdf(dtm)
inspect(dtm_tfxidf[1:10, 5001:5010])
## do document clustering
### k-means (this uses euclidean distance)
m <- as.matrix(dtm_tfxidf)
rownames(m) <- 1:nrow(m)
### don't forget to normalize the vectors so Euclidean makes sense
norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)
m_norm <- norm_eucl(m)
### cluster into 10 clusters
cl <- kmeans(m_norm, 10)
cl
table(cl$cluster)
### show clusters using the first 2 principal components
plot(prcomp(m_norm)$x, col=cl$cl)
findFreqTerms(dtm[cl$cluster==1], 50)
inspect(reuters[which(cl$cluster==1)])
## hierarchical clustering
library(proxy)
### this is going to take 4-ever (O(n^2))
d <- dist(m, method="cosine")
hc <- hclust(d, method="average")
plot(hc)
cl <- cutree(hc, 50)
table(cl)
findFreqTerms(dtm[cl==1], 50)
data(Titanic)
Titanic
data(Titanic)
Titani
Titanic
dinnames(Titanic)
dimnames(Titanic)
library(MASS)
indep = loglm(˜Class+Sex+Age+Survived, data=Titanic)
indep = loglm(Class+Sex+Age+Survived, data=Titanic)
indep = loglm(~Class+Sex+Age+Survived, data=Titanic)
indep
modelB = loglm(˜Class*Sex*Age*Survived, data=Titanic)
modelB = loglm(~Class*Sex*Age*Survived, data=Titanic)
modelB
coef(modelB)
modelA = loglm(~Class+Sex+Age+Survived, data=Titanic)
coef(modelA)
round(fitted(modelA), 1)
modelB = loglm(~Class+Sex+Age+Survived+Class:Sex+Sex:Survived,data=Titanic)
coef(modelB)
model.aic = step(modelA, scope=list(lower=~., upper=~.ˆ4))
model.aic = step(modelA, scope=list(lower=~., upper=~.^4))
phat.aic = fitted(model.aic)/sum(Titanic)
round(prop.table(phat.aic, c(1,2,3))[,,,"Yes"], 2)
A<-array(0,dim=c(500,1))
x<-rnorm(25,mean=2,sd=1)
for(i in  1:500){
y<-rnorm(25,mean=(3*x+2),sd=1)
beta<-lm(x ~y)
A[i]<-beta$coef[2]
}
Abar<-mean(A)
varA<-var(A)
Abar
varA
summary(A)
install.packages("forecast")
install.packages("forecast")
irisdata <- read.csv("http://www.heatonresearch.com/dload/data/iris.csv",head=TRUE,sep=",")
irisdata
irisTrainData = sample(1:150,100)
irisValData = setdiff(1:150,irisTrainData)
#irisdata[irisTrainData,]
#irisdata[irisValData,]
#Using a Support Vector Machine (SVM)
library(kernlab)
#radial basis function (RBF) that will be used during training
rbf <- rbfdot(sigma=0.1)
irisSVM <- ksvm(species~.,data=irisdata[irisTrainData,],type="C-bsvc",kernel=rbf,C=10,prob.model=TRUE)
fitted(irisSVM)
predict(irisSVM, irisdata[irisValData,-5], type="probabilities")
library(nnet)
ideal <- class.ind(irisdata$species)
irisANN = nnet(irisdata[irisTrainData,-5], ideal[irisTrainData,], size=10, softmax=TRUE)
predict(irisANN, irisdata[irisValData,-5], type="class")
predict(irisSVM, irisdata[irisValData,-5], type="class")
setwd("C:/Users/IBM_ADMIN/Documents/GitHub/bicimadmap/batch")
data<-read.csv('dataDemand.csv',sep=",",header=TRUE)
require(ggplot2)
require(tabplot)
summary(data)
#Summary
tableplot(data, cex = 1.8)
setwd("C:/Users/IBM_ADMIN/Documents/GitHub/bicimadmap/batch")
data<-read.csv('dataDemand.csv',sep=",",header=TRUE)
View(data)
View(data)
dataModel <- data[,c(2,4,5,7,8,10,18,19)]
View(dataModel)
View(dataModel)
dataModel <- data[,c(2,4,5,8,9,11,14,16,18,19)]
View(dataModel)
View(dataModel)
dataModel <- data[,c(2,4,5,8,10,11,14,16,18,19)]
View(dataModel)
View(dataModel)
dataModel <- data[,c(2,4,5,8,10,12,14,16,18,19,11)]
View(dataModel)
View(dataModel)
splitdf <- function(dataframe, seed=NULL) {
if (!is.null(seed)) set.seed(seed)
index <- 1:nrow(dataframe)
trainindex <- sample(index, trunc(length(index)/2))
trainset <- dataframe[trainindex, ]
testset <- dataframe[-trainindex, ]
list(trainset=trainset,testset=testset)
}
splits <- splitdf(dataModel, seed=808)
train <- splits$trainset
test <- splits$testset
library(randomForest)
clf <- randomForest(train$occupyBikes ~ ., data=train, ntree=20, nodesize=5, mtry=9)
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression in R r^2=", r2, sep=""))
(r2 <- rSquared(test$occupyBikes, test$occupyBikes - predict(clf, test[,c(1:10)])))
clf.fit(train, train.quality)
(mse <- mean((test$occupyBikes - predict(clf, test[,c(1:10)]))^2))
(mse <- mean((test$occupyBikes - predict(clf, test))^2))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression in R r^2=", mse, sep=""))
splits <- splitdf(dataModel, seed=10)
train <- splits$trainset
test <- splits$testset
splitdf <- function(dataframe, seed=NULL) {
if (!is.null(seed)) set.seed(seed)
index <- 1:nrow(dataframe)
trainindex <- sample(index, trunc((length(index)/3)-1)
trainset <- dataframe[trainindex, ]
testset <- dataframe[-trainindex, ]
splitdf <- function(dataframe, seed=NULL) {
if (!is.null(seed)) set.seed(seed)
index <- 1:nrow(dataframe)
trainindex <- sample(index, trunc(length(index)/3))
trainset <- dataframe[trainindex, ]
testset <- dataframe[-trainindex, ]
list(trainset=trainset,testset=testset)
}
splits <- splitdf(dataModel, seed=10)
train <- splits$trainset
splitdf <- function(dataframe, size, seed=NULL) {
if (!is.null(seed)) set.seed(seed)
index <- 1:nrow(dataframe)
testindex <- sample(index, trunc(length(index)/size))
trainset <- dataframe[-testindex, ]
testset <- dataframe[testindex, ]
list(trainset=trainset,testset=testset)
}
splits <- splitdf(dataModel,size=3, seed=10)
train <- splits$trainset
test <- splits$testset
library(randomForest)
clf <- randomForest(train$occupyBikes ~ ., data=train, ntree=20, nodesize=5, mtry=9)
#R2
(r2 <- rSquared(test$occupyBikes, test$occupyBikes - predict(clf, test)))
#MSE
(mse <- mean((test$occupyBikes - predict(clf, test))^2))
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression in R r^2=", mse, sep=""))
splits <- splitdf(dataModel,size=6, seed=10)
train <- splits$trainset
test <- splits$testset
library(randomForest)
clf <- randomForest(train$occupyBikes ~ ., data=train, ntree=20, nodesize=5, mtry=9)
#R2
(r2 <- rSquared(test$occupyBikes, test$occupyBikes - predict(clf, test)))
#MSE
(mse <- mean((test$occupyBikes - predict(clf, test))^2))
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression in R r^2=", mse, sep=""))
splits <- splitdf(dataModel,size=8, seed=10)
train <- splits$trainset
test <- splits$testset
library(randomForest)
clf <- randomForest(train$occupyBikes ~ ., data=train, ntree=20, nodesize=5, mtry=9)
#R2
(r2 <- rSquared(test$occupyBikes, test$occupyBikes - predict(clf, test)))
#MSE
(mse <- mean((test$occupyBikes - predict(clf, test))^2))
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression in R r^2=", mse, sep=""))
clf
print clf
print(clf)
clf <- randomForest(train$occupyBikes ~ ., data=train)
print(clf)
#R2
(r2 <- rSquared(test$occupyBikes, test$occupyBikes - predict(clf, test)))
#MSE
(mse <- mean((test$occupyBikes - predict(clf, test))^2))
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression in R r^2=", mse, sep=""))
par(mfrow = c(1, 2))
varImpPlot(clf, pch = 19)
title(main = "Contribution of predictor variables", ylab = "Predictor variables")
plot(clf)
title(main = " Misclassification error rates ")
legend("topright", inset = 0.05, title = "Activities", c("Error vs Trees"))
par(mfrow = c(1, 2))
varImpPlot(clf, pch = 19)
title(main = "Contribution of predictor variables", ylab = "Predictor variables")
plot(clf)
title(main = " Misclassification error rates ")
splits <- splitdf(dataModel,size=9, seed=10)
train <- splits$trainset
test <- splits$testset
library(randomForest)
clf <- randomForest(train$occupyBikes ~ ., data=train)
print(clf)
#R2
(r2 <- rSquared(test$occupyBikes, test$occupyBikes - predict(clf, test)))
#MSE
(mse <- mean((test$occupyBikes - predict(clf, test))^2))
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression in R r^2=", mse, sep=""))
par(mfrow = c(1, 2))
varImpPlot(clf, pch = 19)
title(main = "Contribution of predictor variables", ylab = "Predictor variables")
plot(clf)
title(main = " Misclassification error rates ")
clf$importance
r2<- 1 - sum((test$occupyBikes-predict(clf, test))^2)/sum((test$occupyBikes-mean(test$occupyBikes))^2)
r2
(mse <- mean((test$occupyBikes - predict(clf, test))^2))
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point()
+ geom_abline(color="red")
+ ggtitle(paste("RandomForest Regression in R r^2=", r2, sep=""))
(r2<- 1 - sum((test$occupyBikes-predict(clf, test))^2)/sum((test$occupyBikes-mean(test$occupyBikes))^2))
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point()
+ geom_abline(color="red")
+ ggtitle(paste("RandomForest Regression in R r^2=", r2, sep=""))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression in R r^2=", r2, sep=""))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression in R r^2=", as.double(r2), sep=""))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression in R r^2=", round(r2,2), sep=""))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression - r^2 = ", round(r2,2), sep=""))
tableplot(data, cex = 1.8)
tableplot(dataModel, cex = 1.8)
clf$importance
data<-read.csv('dataDemand.csv',sep=",",header=TRUE)
View(data)
View(data)
dataModel <- data[,c(2,4,5,8,10,12,14,16,18,19,20,11)]
splits <- splitdf(dataModel,size=10, seed=10)
train <- splits$trainset
test <- splits$testset
library(randomForest)
clf <- randomForest(train$occupyBikes ~ ., data=train)
print(clf)
#R2 and MSE
(r2<- 1 - sum((test$occupyBikes-predict(clf, test))^2)/sum((test$occupyBikes-mean(test$occupyBikes))^2))
(mse <- mean((test$occupyBikes - predict(clf, test))^2))
#Regression Graph
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression - R^2 = ", round(r2,2), sep=""))
View(dataModel)
View(dataModel)
#Trees performance
par(mfrow = c(1, 2))
varImpPlot(clf, pch = 19)
title(main = "Contribution of predictor variables", ylab = "Predictor variables")
plot(clf)
title(main = " Misclassification error rates ")
dataModel <- data[,c(4,5,8,10,12,14,16,18,19,20,11)]
tableplot(dataModel, cex = 1.8)
# splitdf function will return a list of training and testing sets
splitdf <- function(dataframe, size, seed=NULL) {
if (!is.null(seed)) set.seed(seed)
index <- 1:nrow(dataframe)
testindex <- sample(index, trunc(length(index)/size))
trainset <- dataframe[-testindex, ]
testset <- dataframe[testindex, ]
list(trainset=trainset,testset=testset)
}
splits <- splitdf(dataModel,size=10, seed=10)
train <- splits$trainset
test <- splits$testset
library(randomForest)
clf <- randomForest(train$occupyBikes ~ ., data=train)
print(clf)
#R2 and MSE
(r2<- 1 - sum((test$occupyBikes-predict(clf, test))^2)/sum((test$occupyBikes-mean(test$occupyBikes))^2))
(mse <- mean((test$occupyBikes - predict(clf, test))^2))
#Regression Graph
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression - R^2 = ", round(r2,2), sep=""))
#Trees performance
par(mfrow = c(1, 2))
varImpPlot(clf, pch = 19)
title(main = "Contribution of predictor variables", ylab = "Predictor variables")
plot(clf)
title(main = " Misclassification error rates ")
dataModel <- data[,c(4,5,8,10,12,16,18,19,20,11)]
splits <- splitdf(dataModel,size=10, seed=10)
train <- splits$trainset
test <- splits$testset
clf <- randomForest(train$occupyBikes ~ ., data=train)
print(clf)
#R2 and MSE
(r2<- 1 - sum((test$occupyBikes-predict(clf, test))^2)/sum((test$occupyBikes-mean(test$occupyBikes))^2))
(mse <- mean((test$occupyBikes - predict(clf, test))^2))
#Regression Graph
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression - R^2 = ", round(r2,2), sep=""))
#Trees performance
par(mfrow = c(1, 2))
varImpPlot(clf, pch = 19)
title(main = "Contribution of predictor variables", ylab = "Predictor variables")
plot(clf)
source('~/GitHub/bicimadmap/batch/model.R')
source('~/GitHub/bicimadmap/batch/model.R')
install.packages("pmml")
library(pmml)
sink("bicimadmapPredictionModel.pmml")
cat("<?xml version=\"1.0\"?>\n")
pmml(clf)
clf <- randomForest(train$occupyBikes ~ ., data=train,ntree=20, nodesize=5, mtry=9))
clf <- randomForest(train$occupyBikes ~ ., data=train,ntree=20, nodesize=5, mtry=9)
print(clf)
#R2 and MSE
(r2<- 1 - sum((test$occupyBikes-predict(clf, test))^2)/sum((test$occupyBikes-mean(test$occupyBikes))^2))
(mse <- mean((test$occupyBikes - predict(clf, test))^2))
#Regression Graph
p <- ggplot(aes(x=actual, y=pred),data=data.frame(actual=test$occupyBikes, pred=predict(clf, test)))
p + geom_point() +
geom_abline(color="red") +
ggtitle(paste("RandomForest Regression - R^2 = ", round(r2,2), sep=""))
#Trees performance
par(mfrow = c(1, 2))
varImpPlot(clf, pch = 19)
title(main = "Contribution of predictor variables", ylab = "Predictor variables")
plot(clf)
title(main = " Misclassification error rates ")
library(pmml)
sink("bicimadmapPredictionModel.pmml")
cat("<?xml version=\"1.0\"?>\n")
pmml(clf)
setwd("C:/Users/IBM_ADMIN/Documents/GitHub/bicimadmap/batch")
source('~/.active-rstudio-document')
sink("bicimadmapPredictionModel.pmml")
cat("<?xml version=\"1.0\"?>\n")
pmml(clf)
sink()
